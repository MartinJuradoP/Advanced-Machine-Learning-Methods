{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team:\n",
    "\n",
    "- **Martín Jurado** .................................................. *A01795568*\n",
    "- **Lineth Guerra** .................................................. *A01795639*\n",
    "- **Erick Eduardo Betancourt** ........................... *A01795545*\n",
    "- **Luis Angel González Castellano** ................. *A01795481*\n",
    "- **Jesus Armando Anaya Orozco** .................... *A01795464*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#################################\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/media/pepe/DataUbuntu/Databases/asl_data/'\n",
    "DATA_PATH = './asl_data'\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(DATA_PATH, 'sign_mnist_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     12     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
    "    '''\n",
    "    Function to split the validation set into validation and test subsets.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy array, features of the dataset.\n",
    "    - y: numpy array, corresponding labels.\n",
    "    - pct: float, the percentage of the dataset to be used for testing.\n",
    "    - shuffle: boolean, whether the data should be shuffled before splitting.\n",
    "\n",
    "    Returns:\n",
    "    - x_val: numpy array, validation subset of the features.\n",
    "    - y_val: numpy array, validation subset of the labels.\n",
    "    - x_test: numpy array, test subset of the features.\n",
    "    - y_test: numpy array, test subset of the labels.\n",
    "    '''\n",
    "    assert 0 < pct < 1, \"The percentage must be between 0 and 1\"\n",
    "    \n",
    "    # Shuffle the data if shuffle is True\n",
    "    if shuffle:\n",
    "        indices = np.arange(x.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "    \n",
    "    # Determine the size of the test subset\n",
    "    test_size = int(x.shape[0] * pct)\n",
    "    \n",
    "    # Split the dataset into validation and test subsets\n",
    "    x_test = x[:test_size]\n",
    "    y_test = y[:test_size]\n",
    "    x_val = x[test_size:]\n",
    "    y_val = y[test_size:]\n",
    "    \n",
    "    return x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val, x_test, y_test = split_val_test(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "### The following\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "alphabet.remove('j')\n",
    "alphabet.remove('z')\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mean: 0.000004, Normalized Std: 0.999999\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation of the training data\n",
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "def normalise(x_mean, x_std, x_data):\n",
    "    \"\"\"\n",
    "    Normalize the input data using the provided mean and standard deviation.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_mean: float, the mean of the dataset.\n",
    "    - x_std: float, the standard deviation of the dataset.\n",
    "    - x_data: numpy array, the data to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array, the normalized data.\n",
    "    \"\"\"\n",
    "    return (x_data - x_mean) / x_std\n",
    "\n",
    "# Apply normalization to training, validation, and test datasets\n",
    "# Using the mean and std calculated from the training data\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)\n",
    "\n",
    "# Check the mean and standard deviation of the normalized training data\n",
    "normalized_mean = x_train.mean()\n",
    "normalized_std = x_train.std()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Normalized Mean: {normalized_mean:.6f}, Normalized Std: {normalized_std:.6f}\")  # This should be approximately 0 and 1, respectively\n",
    "\n",
    "# Expected results\n",
    "expected_mean = np.float32(-2.4830147e-08)  # Close to 0\n",
    "expected_std = np.float32(1.0)  # Close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sampled image represents: m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot a given image\n",
    "def plot_number(image):\n",
    "    plt.figure(figsize=(5, 5))  # Set the figure size\n",
    "    plt.imshow(image.squeeze(), cmap='gray')  # Display the image in grayscale\n",
    "    plt.axis('off')  # Turn off the axis to have a cleaner view\n",
    "    plt.show()  # Show the plotted image\n",
    "\n",
    "# Randomly sample an index from the test set\n",
    "rnd_idx = np.random.randint(len(y_test))  # Select a random index from the test labels\n",
    "print(f'The sampled image represents: {alphabet[y_test[rnd_idx]]}')  # Print the corresponding label by the Alphabet\n",
    "\n",
    "# Reshape the image if necessary from 784 to 28x28\n",
    "image_to_plot = x_test[rnd_idx].reshape(28, 28)  # Reshape to the correct dimensions for a 28x28 image\n",
    "\n",
    "# Plot the image corresponding to the randomly selected index from the test set\n",
    "plot_number(image_to_plot)  # Use the reshaped image to display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    '''\n",
    "    Create mini-batches from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - mb_size: int, size of each mini-batch.\n",
    "    - x: numpy array, input data with shape (num_samples, 784).\n",
    "    - y: numpy array, labels corresponding to the input data with shape (num_samples, 1).\n",
    "    - shuffle: bool, whether to shuffle the data before creating mini-batches.\n",
    "    \n",
    "    Returns:\n",
    "    - A generator that yields mini-batches of (x_batch, y_batch).\n",
    "    '''\n",
    "    \n",
    "    # Assert that the number of samples in x and y are the same\n",
    "    assert x.shape[0] == y.shape[0], 'Error: Number of samples in x and y must be equal.'\n",
    "    \n",
    "    # Get the total number of samples in the dataset\n",
    "    total_data = x.shape[0]\n",
    "    \n",
    "    # If shuffle is True, shuffle the dataset\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    \n",
    "    # Generate mini-batches by slicing the dataset\n",
    "    return ((x[i:i + mb_size], y[i:i + mb_size]) for i in range(0, total_data, mb_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    '''\n",
    "    x  #muestras, 784\n",
    "    y #muestras, 1\n",
    "    '''\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of a: <class 'numpy.ndarray'>\n",
      "Type of b: <class '__main__.np_tensor'>\n",
      "a == b: [ True  True]\n",
      "a is b: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class np_tensor(np.ndarray):\n",
    "    pass\n",
    "\n",
    "# Create a NumPy array and view it as a np_tensor\n",
    "a = np.array([0, 0])      # Create a standard NumPy array\n",
    "b = a.view(np_tensor)      # View 'a' as an instance of np_tensor\n",
    "\n",
    "# Check the types\n",
    "print(f'Type of a: {type(a)}')  # Should be numpy.ndarray\n",
    "print(f'Type of b: {type(b)}')  # Should be np_tensor\n",
    "\n",
    "# Check equality and reference\n",
    "print(f'a == b: {a == b}')  # Content comparison\n",
    "print(f'a is b: {a is b}')   # Reference comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Initialize parameters using He initialization for ReLU activation.\n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - output_size: Number of output features.\n",
    "        '''\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)  # Weights\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)  # Biases\n",
    "\n",
    "    def __call__(self, X):  # Forward pass\n",
    "        Z = self.W @ X + self.b  # Linear transformation\n",
    "        return Z\n",
    "\n",
    "    def backward(self, X, Z):\n",
    "        # Backward pass to compute gradients\n",
    "        X.grad = self.W.T @ Z.grad  # Gradient for input\n",
    "        self.W.grad = Z.grad @ X.T  # Gradient for weights\n",
    "        self.b.grad = np.sum(Z.grad, axis=1, keepdims=True)  # Gradient for bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, Z):\n",
    "        # Forward pass using ReLU activation\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z, A):\n",
    "        # Backward pass for ReLU\n",
    "        Z.grad = A.grad.copy()  # Gradient from the next layer\n",
    "        Z.grad[Z <= 0] = 0  # Set gradient to 0 for negative inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_layers():\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        Initialize the sequential model with layers.\n",
    "        Parameters:\n",
    "        - layers: List of layers (Linear, ReLU, etc.)\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.x = None\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self, X):\n",
    "        self.x = X\n",
    "        self.outputs['l0'] = self.x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            self.x = layer(self.x)  # Forward pass through each layer\n",
    "            self.outputs['l'+str(i)] = self.x\n",
    "        return self.x\n",
    "\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])  # Backward pass through layers\n",
    "\n",
    "    def update(self, learning_rate=1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ReLU): \n",
    "                continue  # Skip updating ReLU layers\n",
    "            layer.W -= learning_rate * layer.W.grad  # Update weights\n",
    "            layer.b -= learning_rate * layer.b.grad  # Update biases\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.__call__(X))  # Get predicted class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    \"\"\"\n",
    "    Compute softmax and cross-entropy cost.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Raw scores from the model (logits).\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns:\n",
    "    - preds: Predicted probabilities for each class.\n",
    "    - cost: Average cross-entropy cost for the batch.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[1]  # Number of samples in the batch\n",
    "    exp_scores = np.exp(x)  # Exponentiate the scores\n",
    "    probs = exp_scores / exp_scores.sum(axis=0)  # Normalize to get probabilities\n",
    "\n",
    "    # Calculate the predicted probabilities for the true labels\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size  # Average cross-entropy cost\n",
    "\n",
    "    # Compute gradients for backpropagation\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # Gradient of the loss w.r.t logits\n",
    "    x.grad = probs.copy()  # Store gradients for backpropagation\n",
    "\n",
    "    return probs, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model and compute accuracy and cost\n",
    "def train(model, epochs, mb_size=10, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    train\n",
    "\n",
    "    Trains the provided model using mini-batch gradient descent over a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to be trained.\n",
    "    - epochs (int): The number of complete passes through the training dataset.\n",
    "    - mb_size (int, optional): The size of each mini-batch. Default is 10.\n",
    "    - learning_rate (float, optional): The learning rate for updating model weights. Default is 1e-3.\n",
    "\n",
    "    Returns:\n",
    "    - val_accuracy (float): The final validation accuracy after training.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            scores = model(x.T.view(np_tensor))  # Forward pass\n",
    "            _, cost = softmaxXEntropy(scores, y)  # Compute cost\n",
    "            model.backward()  # Backward pass\n",
    "            model.update(learning_rate)  # Update weights and biases\n",
    "        \n",
    "        # Print cost and accuracy at the end of each epoch\n",
    "        val_accuracy = accuracy(x_val, y_val, mb_size)\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Cost: {cost}, Accuracy: {val_accuracy}')\n",
    "    \n",
    "    # Return the final validation accuracy\n",
    "    return val_accuracy\n",
    "\n",
    "def accuracy(x, y, mb_size):\n",
    "    \"\"\"\n",
    "    accuracy\n",
    "\n",
    "    Computes the accuracy of the model's predictions against the true labels.\n",
    "\n",
    "    Parameters:\n",
    "    - x (ndarray): Input data to evaluate predictions.\n",
    "    - y (ndarray): True labels corresponding to the input data.\n",
    "    - mb_size (int): The size of each mini-batch for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - (float): The computed accuracy as a fraction of correct predictions to total predictions.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(create_minibatches(mb_size, x, y)):\n",
    "        pred = model(x_batch.T.view(np_tensor))  # Get predictions\n",
    "        correct += np.sum(np.argmax(pred, axis=0) == y_batch.squeeze())  # Count correct predictions\n",
    "        total += pred.shape[1]  # Total predictions\n",
    "    return correct / total  # Return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with mb_size: 32, learning_rate: 0.0001, epochs: 60\n",
      "Epoch 1/60 - Cost: 0.8378506248971389, Accuracy: 0.620468488566648\n",
      "Epoch 2/60 - Cost: 0.5003631495159165, Accuracy: 0.7069157836029002\n",
      "Epoch 3/60 - Cost: 0.1660607298730207, Accuracy: 0.7281093139988846\n",
      "Epoch 4/60 - Cost: 0.08754882052623317, Accuracy: 0.741494701617401\n",
      "Epoch 5/60 - Cost: 0.05541973649618595, Accuracy: 0.7498605688789738\n",
      "Epoch 6/60 - Cost: 0.039390665188220333, Accuracy: 0.7554378137200223\n",
      "Epoch 7/60 - Cost: 0.022363476305166097, Accuracy: 0.7579475738984941\n",
      "Epoch 8/60 - Cost: 0.033373984641936576, Accuracy: 0.7576687116564417\n",
      "Epoch 9/60 - Cost: 0.019226367769528664, Accuracy: 0.7632459564974903\n",
      "Epoch 10/60 - Cost: 0.013298890441517575, Accuracy: 0.7649191299498048\n",
      "Epoch 11/60 - Cost: 0.014008491521143895, Accuracy: 0.7663134411600669\n",
      "Epoch 12/60 - Cost: 0.01692215668010748, Accuracy: 0.7688232013385388\n",
      "Epoch 13/60 - Cost: 0.007645605978425878, Accuracy: 0.7685443390964863\n",
      "Epoch 14/60 - Cost: 0.010684830075697201, Accuracy: 0.7707752370329057\n",
      "Epoch 15/60 - Cost: 0.008369649444386843, Accuracy: 0.7691020635805912\n",
      "Epoch 16/60 - Cost: 0.00982871974213478, Accuracy: 0.7704963747908533\n",
      "Epoch 17/60 - Cost: 0.004846935503792119, Accuracy: 0.7688232013385388\n",
      "Epoch 18/60 - Cost: 0.007040361367842586, Accuracy: 0.7721695482431679\n",
      "Epoch 19/60 - Cost: 0.010067880928567762, Accuracy: 0.7730061349693251\n",
      "Epoch 20/60 - Cost: 0.005867844052711445, Accuracy: 0.7718906860011154\n",
      "Epoch 21/60 - Cost: 0.007403908130053626, Accuracy: 0.771611823759063\n",
      "Epoch 22/60 - Cost: 0.005630504677030408, Accuracy: 0.7713329615170106\n",
      "Epoch 23/60 - Cost: 0.005413194440801674, Accuracy: 0.7741215839375348\n",
      "Epoch 24/60 - Cost: 0.004030228548919696, Accuracy: 0.7741215839375348\n",
      "Epoch 25/60 - Cost: 0.004219153466298016, Accuracy: 0.7752370329057445\n",
      "Epoch 26/60 - Cost: 0.004338289591155287, Accuracy: 0.7727272727272727\n",
      "Epoch 27/60 - Cost: 0.002675479003162445, Accuracy: 0.7732849972113776\n",
      "Epoch 28/60 - Cost: 0.0026579591040893037, Accuracy: 0.7749581706636921\n",
      "Epoch 29/60 - Cost: 0.004329309725817655, Accuracy: 0.775515895147797\n",
      "Epoch 30/60 - Cost: 0.0036171455383876617, Accuracy: 0.775515895147797\n",
      "Epoch 31/60 - Cost: 0.003926214531798842, Accuracy: 0.775515895147797\n",
      "Epoch 32/60 - Cost: 0.0030064093701916415, Accuracy: 0.7780256553262688\n",
      "Epoch 33/60 - Cost: 0.0031263252621590576, Accuracy: 0.7769102063580591\n",
      "Epoch 34/60 - Cost: 0.0023092053204115657, Accuracy: 0.7774679308421639\n",
      "Epoch 35/60 - Cost: 0.0021908231807890566, Accuracy: 0.7771890686001115\n",
      "Epoch 36/60 - Cost: 0.004519425997719196, Accuracy: 0.7763524818739542\n",
      "Epoch 37/60 - Cost: 0.0017659186246738717, Accuracy: 0.7774679308421639\n",
      "Epoch 38/60 - Cost: 0.002186481627040649, Accuracy: 0.7771890686001115\n",
      "Epoch 39/60 - Cost: 0.0028421279498292984, Accuracy: 0.7774679308421639\n",
      "Epoch 40/60 - Cost: 0.0030462609071476277, Accuracy: 0.7788622420524262\n",
      "Epoch 41/60 - Cost: 0.0026474216768968997, Accuracy: 0.7783045175683212\n",
      "Epoch 42/60 - Cost: 0.002750610222447392, Accuracy: 0.7791411042944786\n",
      "Epoch 43/60 - Cost: 0.002890916178365987, Accuracy: 0.7783045175683212\n",
      "Epoch 44/60 - Cost: 0.001974542292774145, Accuracy: 0.7791411042944786\n",
      "Epoch 45/60 - Cost: 0.0015750492939581352, Accuracy: 0.7785833798103736\n",
      "Epoch 46/60 - Cost: 0.002237376689696971, Accuracy: 0.7785833798103736\n",
      "Epoch 47/60 - Cost: 0.0021655073317695, Accuracy: 0.7799776910206359\n",
      "Epoch 48/60 - Cost: 0.0016743812513934262, Accuracy: 0.7799776910206359\n",
      "Epoch 49/60 - Cost: 0.0010319819505457781, Accuracy: 0.7796988287785834\n",
      "Epoch 50/60 - Cost: 0.002789695487383557, Accuracy: 0.7791411042944786\n",
      "Epoch 51/60 - Cost: 0.0018800865799931824, Accuracy: 0.7802565532626883\n",
      "Epoch 52/60 - Cost: 0.001746175343450319, Accuracy: 0.7805354155047407\n",
      "Epoch 53/60 - Cost: 0.0029651060381409157, Accuracy: 0.7796988287785834\n",
      "Epoch 54/60 - Cost: 0.001290550426382388, Accuracy: 0.7805354155047407\n",
      "Epoch 55/60 - Cost: 0.001576599848001493, Accuracy: 0.7799776910206359\n",
      "Epoch 56/60 - Cost: 0.002039845460979679, Accuracy: 0.7799776910206359\n",
      "Epoch 57/60 - Cost: 0.0017220500017588317, Accuracy: 0.7805354155047407\n",
      "Epoch 58/60 - Cost: 0.002238920644388342, Accuracy: 0.7802565532626883\n",
      "Epoch 59/60 - Cost: 0.0018291488984330075, Accuracy: 0.7802565532626883\n",
      "Epoch 60/60 - Cost: 0.001943549883949033, Accuracy: 0.781372002230898\n",
      "Training with mb_size: 64, learning_rate: 0.0005, epochs: 90\n",
      "Epoch 1/90 - Cost: 0.08333300432827989, Accuracy: 0.7732849972113776\n",
      "Epoch 2/90 - Cost: 0.0179177344084918, Accuracy: 0.7802565532626883\n",
      "Epoch 3/90 - Cost: 0.00955017113585567, Accuracy: 0.7799776910206359\n",
      "Epoch 4/90 - Cost: 0.007349294219168045, Accuracy: 0.7788622420524262\n",
      "Epoch 5/90 - Cost: 0.006740719956016977, Accuracy: 0.781372002230898\n",
      "Epoch 6/90 - Cost: 0.004264443995013466, Accuracy: 0.7805354155047407\n",
      "Epoch 7/90 - Cost: 0.002841238002702075, Accuracy: 0.7838817624093698\n",
      "Epoch 8/90 - Cost: 0.0027259044484689394, Accuracy: 0.781372002230898\n",
      "Epoch 9/90 - Cost: 0.0023940334305537258, Accuracy: 0.7858337981037368\n",
      "Epoch 10/90 - Cost: 0.0015316805210765518, Accuracy: 0.7869492470719465\n",
      "Epoch 11/90 - Cost: 0.0016569591778173409, Accuracy: 0.7869492470719465\n",
      "Epoch 12/90 - Cost: 0.0018998328450324144, Accuracy: 0.7872281093139989\n",
      "Epoch 13/90 - Cost: 0.0009670867172029877, Accuracy: 0.7869492470719465\n",
      "Epoch 14/90 - Cost: 0.0010818131668956783, Accuracy: 0.7883435582822086\n",
      "Epoch 15/90 - Cost: 0.0006219983901162511, Accuracy: 0.7908533184606804\n",
      "Epoch 16/90 - Cost: 0.000720235600547739, Accuracy: 0.7875069715560513\n",
      "Epoch 17/90 - Cost: 0.0006721885748355595, Accuracy: 0.7875069715560513\n",
      "Epoch 18/90 - Cost: 0.0007886691677532839, Accuracy: 0.7933630786391522\n",
      "Epoch 19/90 - Cost: 0.0006631232000445342, Accuracy: 0.7891801450083659\n",
      "Epoch 20/90 - Cost: 0.0004697364022309086, Accuracy: 0.7900167317345231\n",
      "Epoch 21/90 - Cost: 0.0007302317984375371, Accuracy: 0.7897378694924707\n",
      "Epoch 22/90 - Cost: 0.0006639378564027656, Accuracy: 0.792526491912995\n",
      "Epoch 23/90 - Cost: 0.0006152135721784188, Accuracy: 0.7911321807027328\n",
      "Epoch 24/90 - Cost: 0.0005539202641212191, Accuracy: 0.7953151143335192\n",
      "Epoch 25/90 - Cost: 0.0005663372016407705, Accuracy: 0.7922476296709425\n",
      "Epoch 26/90 - Cost: 0.0004911345896900393, Accuracy: 0.7928053541550474\n",
      "Epoch 27/90 - Cost: 0.0005687262442490498, Accuracy: 0.7936419408812047\n",
      "Epoch 28/90 - Cost: 0.0007408087715179958, Accuracy: 0.7930842163970998\n",
      "Epoch 29/90 - Cost: 0.0005804269825557907, Accuracy: 0.7936419408812047\n",
      "Epoch 30/90 - Cost: 0.00035910548374187536, Accuracy: 0.7930842163970998\n",
      "Epoch 31/90 - Cost: 0.0004633352446383897, Accuracy: 0.7908533184606804\n",
      "Epoch 32/90 - Cost: 0.00040460900775679635, Accuracy: 0.7941996653653095\n",
      "Epoch 33/90 - Cost: 0.0002901507249487159, Accuracy: 0.7936419408812047\n",
      "Epoch 34/90 - Cost: 0.0006295965120654924, Accuracy: 0.7930842163970998\n",
      "Epoch 35/90 - Cost: 0.0003390231682529861, Accuracy: 0.7936419408812047\n",
      "Epoch 36/90 - Cost: 0.00035211542389026, Accuracy: 0.7936419408812047\n",
      "Epoch 37/90 - Cost: 0.0004906896026912593, Accuracy: 0.7955939765755716\n",
      "Epoch 38/90 - Cost: 0.0005321803484907997, Accuracy: 0.7930842163970998\n",
      "Epoch 39/90 - Cost: 0.0003050466376618997, Accuracy: 0.7936419408812047\n",
      "Epoch 40/90 - Cost: 0.00037474900095685825, Accuracy: 0.7953151143335192\n",
      "Epoch 41/90 - Cost: 0.00046606760761282715, Accuracy: 0.7947573898494144\n",
      "Epoch 42/90 - Cost: 0.0005108865158979268, Accuracy: 0.7941996653653095\n",
      "Epoch 43/90 - Cost: 0.0002071114427263795, Accuracy: 0.795872838817624\n",
      "Epoch 44/90 - Cost: 0.00023688500530192363, Accuracy: 0.7941996653653095\n",
      "Epoch 45/90 - Cost: 0.0005975354449093446, Accuracy: 0.7944785276073619\n",
      "Epoch 46/90 - Cost: 0.00023679857262047624, Accuracy: 0.7941996653653095\n",
      "Epoch 47/90 - Cost: 0.00029154556382199326, Accuracy: 0.7964305633017289\n",
      "Epoch 48/90 - Cost: 0.00026766606545558286, Accuracy: 0.7950362520914668\n",
      "Epoch 49/90 - Cost: 0.0002887970744710247, Accuracy: 0.7950362520914668\n",
      "Epoch 50/90 - Cost: 0.0003473251440809267, Accuracy: 0.7961517010596765\n",
      "Epoch 51/90 - Cost: 0.0002774895004721107, Accuracy: 0.7950362520914668\n",
      "Epoch 52/90 - Cost: 0.00018370422713170684, Accuracy: 0.7941996653653095\n",
      "Epoch 53/90 - Cost: 0.00034084937945614283, Accuracy: 0.7955939765755716\n",
      "Epoch 54/90 - Cost: 0.0001909136591306518, Accuracy: 0.7953151143335192\n",
      "Epoch 55/90 - Cost: 0.00026682408421245323, Accuracy: 0.7939208031232571\n",
      "Epoch 56/90 - Cost: 0.00018081689864083003, Accuracy: 0.7955939765755716\n",
      "Epoch 57/90 - Cost: 0.0003171602141710579, Accuracy: 0.7936419408812047\n",
      "Epoch 58/90 - Cost: 0.0001505302107595717, Accuracy: 0.7950362520914668\n",
      "Epoch 59/90 - Cost: 0.00014350431991023795, Accuracy: 0.7944785276073619\n",
      "Epoch 60/90 - Cost: 0.00018431711978645832, Accuracy: 0.7941996653653095\n",
      "Epoch 61/90 - Cost: 0.00024071516922901295, Accuracy: 0.7944785276073619\n",
      "Epoch 62/90 - Cost: 0.00022309064320790556, Accuracy: 0.7947573898494144\n",
      "Epoch 63/90 - Cost: 0.00018539851306025956, Accuracy: 0.7950362520914668\n",
      "Epoch 64/90 - Cost: 0.00024270353546554914, Accuracy: 0.7941996653653095\n",
      "Epoch 65/90 - Cost: 0.0002654792870309653, Accuracy: 0.7953151143335192\n",
      "Epoch 66/90 - Cost: 0.00019731621553483475, Accuracy: 0.7947573898494144\n",
      "Epoch 67/90 - Cost: 0.00015284493293396932, Accuracy: 0.7950362520914668\n",
      "Epoch 68/90 - Cost: 0.0001318590794234291, Accuracy: 0.7944785276073619\n",
      "Epoch 69/90 - Cost: 0.00038576681863322155, Accuracy: 0.7944785276073619\n",
      "Epoch 70/90 - Cost: 0.00018098478561421499, Accuracy: 0.7950362520914668\n",
      "Epoch 71/90 - Cost: 0.00016812797753868776, Accuracy: 0.795872838817624\n",
      "Epoch 72/90 - Cost: 0.00017999352999415854, Accuracy: 0.7947573898494144\n",
      "Epoch 73/90 - Cost: 0.0001396054593137117, Accuracy: 0.7941996653653095\n",
      "Epoch 74/90 - Cost: 0.00020396293679866473, Accuracy: 0.7953151143335192\n",
      "Epoch 75/90 - Cost: 0.00019286231252493909, Accuracy: 0.7944785276073619\n",
      "Epoch 76/90 - Cost: 0.00018260182115454083, Accuracy: 0.7950362520914668\n",
      "Epoch 77/90 - Cost: 0.00022672763672300762, Accuracy: 0.7953151143335192\n",
      "Epoch 78/90 - Cost: 0.00019383941962837327, Accuracy: 0.7953151143335192\n",
      "Epoch 79/90 - Cost: 0.00015283626045562327, Accuracy: 0.7950362520914668\n",
      "Epoch 80/90 - Cost: 0.00015893451199492967, Accuracy: 0.7955939765755716\n",
      "Epoch 81/90 - Cost: 0.00013866362861504099, Accuracy: 0.7950362520914668\n",
      "Epoch 82/90 - Cost: 0.00010493319936584579, Accuracy: 0.7950362520914668\n",
      "Epoch 83/90 - Cost: 0.00018886450374984055, Accuracy: 0.7950362520914668\n",
      "Epoch 84/90 - Cost: 0.00014361231731518673, Accuracy: 0.7947573898494144\n",
      "Epoch 85/90 - Cost: 0.0001446489317196242, Accuracy: 0.7953151143335192\n",
      "Epoch 86/90 - Cost: 0.00012055656415602115, Accuracy: 0.7947573898494144\n",
      "Epoch 87/90 - Cost: 0.000146926998019546, Accuracy: 0.7950362520914668\n",
      "Epoch 88/90 - Cost: 0.0001186789211235551, Accuracy: 0.7953151143335192\n",
      "Epoch 89/90 - Cost: 0.00015366260542081818, Accuracy: 0.7950362520914668\n",
      "Epoch 90/90 - Cost: 0.00011994164689627736, Accuracy: 0.7950362520914668\n",
      "Training with mb_size: 64, learning_rate: 0.0003, epochs: 120\n",
      "Epoch 1/120 - Cost: 0.20483876112617522, Accuracy: 0.7138873396542108\n",
      "Epoch 2/120 - Cost: 0.08028731064455974, Accuracy: 0.7621305075292806\n",
      "Epoch 3/120 - Cost: 0.034567204383019495, Accuracy: 0.7788622420524262\n",
      "Epoch 4/120 - Cost: 0.015295818972541092, Accuracy: 0.7802565532626883\n",
      "Epoch 5/120 - Cost: 0.011524169804484393, Accuracy: 0.7824874511991077\n",
      "Epoch 6/120 - Cost: 0.0062769920060721475, Accuracy: 0.7830451756832125\n",
      "Epoch 7/120 - Cost: 0.004324584747198907, Accuracy: 0.7872281093139989\n",
      "Epoch 8/120 - Cost: 0.005714864274638777, Accuracy: 0.7816508644729504\n",
      "Epoch 9/120 - Cost: 0.00426372971741405, Accuracy: 0.7841606246514222\n",
      "Epoch 10/120 - Cost: 0.0025977277881164772, Accuracy: 0.786670384829894\n",
      "Epoch 11/120 - Cost: 0.002325847088082807, Accuracy: 0.7847183491355271\n",
      "Epoch 12/120 - Cost: 0.0031653019026397196, Accuracy: 0.7841606246514222\n",
      "Epoch 13/120 - Cost: 0.003915928891935062, Accuracy: 0.7875069715560513\n",
      "Epoch 14/120 - Cost: 0.0035628457673501307, Accuracy: 0.7855549358616843\n",
      "Epoch 15/120 - Cost: 0.0021713140424770394, Accuracy: 0.7872281093139989\n",
      "Epoch 16/120 - Cost: 0.0022583966784859334, Accuracy: 0.7869492470719465\n",
      "Epoch 17/120 - Cost: 0.002228726394581552, Accuracy: 0.7883435582822086\n",
      "Epoch 18/120 - Cost: 0.0020805500473234048, Accuracy: 0.7883435582822086\n",
      "Epoch 19/120 - Cost: 0.0027987446009029647, Accuracy: 0.7875069715560513\n",
      "Epoch 20/120 - Cost: 0.0013333044315466292, Accuracy: 0.7880646960401562\n",
      "Epoch 21/120 - Cost: 0.0017665419320677589, Accuracy: 0.7883435582822086\n",
      "Epoch 22/120 - Cost: 0.0009642822395690787, Accuracy: 0.788622420524261\n",
      "Epoch 23/120 - Cost: 0.0012753668973797556, Accuracy: 0.7883435582822086\n",
      "Epoch 24/120 - Cost: 0.0011866089880503406, Accuracy: 0.7877858337981037\n",
      "Epoch 25/120 - Cost: 0.0009811858386143727, Accuracy: 0.7880646960401562\n",
      "Epoch 26/120 - Cost: 0.0008573068171270513, Accuracy: 0.7877858337981037\n",
      "Epoch 27/120 - Cost: 0.0011721939417283585, Accuracy: 0.788622420524261\n",
      "Epoch 28/120 - Cost: 0.0011500526821406885, Accuracy: 0.7897378694924707\n",
      "Epoch 29/120 - Cost: 0.0011066646979931009, Accuracy: 0.7891801450083659\n",
      "Epoch 30/120 - Cost: 0.0008696989883143004, Accuracy: 0.7897378694924707\n",
      "Epoch 31/120 - Cost: 0.0005452876461931712, Accuracy: 0.7897378694924707\n",
      "Epoch 32/120 - Cost: 0.001196586117556075, Accuracy: 0.7889012827663134\n",
      "Epoch 33/120 - Cost: 0.0009106722615651822, Accuracy: 0.7897378694924707\n",
      "Epoch 34/120 - Cost: 0.0005290697971667911, Accuracy: 0.7897378694924707\n",
      "Epoch 35/120 - Cost: 0.0005890398793055244, Accuracy: 0.7897378694924707\n",
      "Epoch 36/120 - Cost: 0.000661143763840076, Accuracy: 0.7894590072504183\n",
      "Epoch 37/120 - Cost: 0.0008086501354164096, Accuracy: 0.7897378694924707\n",
      "Epoch 38/120 - Cost: 0.0005207373403596372, Accuracy: 0.7894590072504183\n",
      "Epoch 39/120 - Cost: 0.0006650353048276725, Accuracy: 0.788622420524261\n",
      "Epoch 40/120 - Cost: 0.0006605283101710798, Accuracy: 0.7897378694924707\n",
      "Epoch 41/120 - Cost: 0.0009933868374552976, Accuracy: 0.7900167317345231\n",
      "Epoch 42/120 - Cost: 0.0006858432165731791, Accuracy: 0.7894590072504183\n",
      "Epoch 43/120 - Cost: 0.0007502100543107323, Accuracy: 0.7902955939765756\n",
      "Epoch 44/120 - Cost: 0.0007156681708728785, Accuracy: 0.7900167317345231\n",
      "Epoch 45/120 - Cost: 0.0004295388495074261, Accuracy: 0.7902955939765756\n",
      "Epoch 46/120 - Cost: 0.00048393217744245934, Accuracy: 0.7900167317345231\n",
      "Epoch 47/120 - Cost: 0.00037307015362218033, Accuracy: 0.7916899051868377\n",
      "Epoch 48/120 - Cost: 0.0005114158854797811, Accuracy: 0.7908533184606804\n",
      "Epoch 49/120 - Cost: 0.0005893627080081205, Accuracy: 0.7897378694924707\n",
      "Epoch 50/120 - Cost: 0.0003938748751030977, Accuracy: 0.7914110429447853\n",
      "Epoch 51/120 - Cost: 0.00048519107133583903, Accuracy: 0.7911321807027328\n",
      "Epoch 52/120 - Cost: 0.0002540896350293108, Accuracy: 0.7900167317345231\n",
      "Epoch 53/120 - Cost: 0.0004015233578966287, Accuracy: 0.7902955939765756\n",
      "Epoch 54/120 - Cost: 0.00046859338182906094, Accuracy: 0.7911321807027328\n",
      "Epoch 55/120 - Cost: 0.0003674427406468888, Accuracy: 0.7911321807027328\n",
      "Epoch 56/120 - Cost: 0.00029836224171484447, Accuracy: 0.7900167317345231\n",
      "Epoch 57/120 - Cost: 0.0004326868585113059, Accuracy: 0.7911321807027328\n",
      "Epoch 58/120 - Cost: 0.00039265524186094886, Accuracy: 0.790574456218628\n",
      "Epoch 59/120 - Cost: 0.00037366165906675447, Accuracy: 0.7908533184606804\n",
      "Epoch 60/120 - Cost: 0.0005337719274432786, Accuracy: 0.7902955939765756\n",
      "Epoch 61/120 - Cost: 0.0003517524206998412, Accuracy: 0.7916899051868377\n",
      "Epoch 62/120 - Cost: 0.00042217068588891975, Accuracy: 0.7908533184606804\n",
      "Epoch 63/120 - Cost: 0.0004618381560701159, Accuracy: 0.7911321807027328\n",
      "Epoch 64/120 - Cost: 0.000456861546246315, Accuracy: 0.7911321807027328\n",
      "Epoch 65/120 - Cost: 0.00020394873565154114, Accuracy: 0.7919687674288901\n",
      "Epoch 66/120 - Cost: 0.00036653123876285234, Accuracy: 0.7919687674288901\n",
      "Epoch 67/120 - Cost: 0.00038472534096727036, Accuracy: 0.7919687674288901\n",
      "Epoch 68/120 - Cost: 0.0003311858519733124, Accuracy: 0.7916899051868377\n",
      "Epoch 69/120 - Cost: 0.00035068909459697457, Accuracy: 0.7914110429447853\n",
      "Epoch 70/120 - Cost: 0.00031924307233761244, Accuracy: 0.7919687674288901\n",
      "Epoch 71/120 - Cost: 0.00040318699755703794, Accuracy: 0.7911321807027328\n",
      "Epoch 72/120 - Cost: 0.00034110137017663204, Accuracy: 0.7914110429447853\n",
      "Epoch 73/120 - Cost: 0.0002900469090115377, Accuracy: 0.7919687674288901\n",
      "Epoch 74/120 - Cost: 0.00028070183027637533, Accuracy: 0.7919687674288901\n",
      "Epoch 75/120 - Cost: 0.0003087441486427738, Accuracy: 0.7919687674288901\n",
      "Epoch 76/120 - Cost: 0.00028119293628773326, Accuracy: 0.792526491912995\n",
      "Epoch 77/120 - Cost: 0.0003399731786058563, Accuracy: 0.7914110429447853\n",
      "Epoch 78/120 - Cost: 0.0003714865168096775, Accuracy: 0.7911321807027328\n",
      "Epoch 79/120 - Cost: 0.0001870815178102571, Accuracy: 0.7914110429447853\n",
      "Epoch 80/120 - Cost: 0.0002692555642407919, Accuracy: 0.7919687674288901\n",
      "Epoch 81/120 - Cost: 0.00028640346245540217, Accuracy: 0.7911321807027328\n",
      "Epoch 82/120 - Cost: 0.000289771414764695, Accuracy: 0.7916899051868377\n",
      "Epoch 83/120 - Cost: 0.0002578880263582938, Accuracy: 0.7914110429447853\n",
      "Epoch 84/120 - Cost: 0.000381258788203316, Accuracy: 0.7916899051868377\n",
      "Epoch 85/120 - Cost: 0.0003110670589864689, Accuracy: 0.7916899051868377\n",
      "Epoch 86/120 - Cost: 0.00019981836974038077, Accuracy: 0.7911321807027328\n",
      "Epoch 87/120 - Cost: 0.0002528724547551696, Accuracy: 0.7916899051868377\n",
      "Epoch 88/120 - Cost: 0.0002455749726123975, Accuracy: 0.7919687674288901\n",
      "Epoch 89/120 - Cost: 0.00025059289841823743, Accuracy: 0.7922476296709425\n",
      "Epoch 90/120 - Cost: 0.00022694698036258926, Accuracy: 0.7922476296709425\n",
      "Epoch 91/120 - Cost: 0.00021350294870049237, Accuracy: 0.7922476296709425\n",
      "Epoch 92/120 - Cost: 0.00027435067849697115, Accuracy: 0.7922476296709425\n",
      "Epoch 93/120 - Cost: 0.0003938115184811229, Accuracy: 0.7928053541550474\n",
      "Epoch 94/120 - Cost: 0.00027747220758132525, Accuracy: 0.7914110429447853\n",
      "Epoch 95/120 - Cost: 0.00014165365127176318, Accuracy: 0.7922476296709425\n",
      "Epoch 96/120 - Cost: 0.0002279122364124775, Accuracy: 0.7914110429447853\n",
      "Epoch 97/120 - Cost: 0.00029721451060032056, Accuracy: 0.7919687674288901\n",
      "Epoch 98/120 - Cost: 0.0002927105143592707, Accuracy: 0.7911321807027328\n",
      "Epoch 99/120 - Cost: 0.00017393609692546786, Accuracy: 0.792526491912995\n",
      "Epoch 100/120 - Cost: 0.00020993133293021, Accuracy: 0.7919687674288901\n",
      "Epoch 101/120 - Cost: 0.00021996458051161765, Accuracy: 0.7922476296709425\n",
      "Epoch 102/120 - Cost: 0.00021288173526895436, Accuracy: 0.7908533184606804\n",
      "Epoch 103/120 - Cost: 0.00018383796410909392, Accuracy: 0.7916899051868377\n",
      "Epoch 104/120 - Cost: 0.0002316902459310524, Accuracy: 0.7928053541550474\n",
      "Epoch 105/120 - Cost: 0.00020942236583870063, Accuracy: 0.7919687674288901\n",
      "Epoch 106/120 - Cost: 0.0002569861811829531, Accuracy: 0.7914110429447853\n",
      "Epoch 107/120 - Cost: 0.00022940008091517925, Accuracy: 0.7928053541550474\n",
      "Epoch 108/120 - Cost: 0.00018373443422964254, Accuracy: 0.7928053541550474\n",
      "Epoch 109/120 - Cost: 0.0002089716821953101, Accuracy: 0.792526491912995\n",
      "Epoch 110/120 - Cost: 0.00018805278682856896, Accuracy: 0.7916899051868377\n",
      "Epoch 111/120 - Cost: 0.0001235685217245322, Accuracy: 0.7922476296709425\n",
      "Epoch 112/120 - Cost: 0.0002134203321540919, Accuracy: 0.7914110429447853\n",
      "Epoch 113/120 - Cost: 0.0001795135068598431, Accuracy: 0.7916899051868377\n",
      "Epoch 114/120 - Cost: 0.00014545639142409637, Accuracy: 0.792526491912995\n",
      "Epoch 115/120 - Cost: 0.0002194443700955234, Accuracy: 0.7928053541550474\n",
      "Epoch 116/120 - Cost: 0.000156321966302151, Accuracy: 0.7928053541550474\n",
      "Epoch 117/120 - Cost: 0.00020096260557968994, Accuracy: 0.7928053541550474\n",
      "Epoch 118/120 - Cost: 0.00027617782597963073, Accuracy: 0.7928053541550474\n",
      "Epoch 119/120 - Cost: 0.00011667721970964747, Accuracy: 0.792526491912995\n",
      "Epoch 120/120 - Cost: 0.000249742540630114, Accuracy: 0.7930842163970998\n",
      "\n",
      "Best model found with mb_size: 64, learning_rate: 0.0005, epochs: 90\n",
      "Best validation accuracy: 0.7950362520914668\n"
     ]
    }
   ],
   "source": [
    "# Function to create a new model\n",
    "def create_model():\n",
    "    return Sequential_layers([\n",
    "        Linear(784, 522), ReLU(),  \n",
    "        Linear(522, 261), ReLU(),  \n",
    "        Linear(261, 130), ReLU(),  \n",
    "        Linear(130, 24)   \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter configurations\n",
    "hyperparameter_configs = [\n",
    "    (32, 1e-4, 60),   # Small minibatch, moderate learning rate, moderate epochs\n",
    "    (64, 5e-4, 90),   # Medium minibatch, moderate learning rate, more epochs\n",
    "    (64, 3e-4, 120)  # Large minibatch, moderate learning rate, many epochs\n",
    "]\n",
    "\n",
    "# Variables to store the best model\n",
    "best_accuracy = 0\n",
    "best_hyperparams = None\n",
    "best_model = None\n",
    "\n",
    "# Iterate over each hyperparameter configuration\n",
    "for mb_size, learning_rate, epochs in hyperparameter_configs:\n",
    "    print(f\"Training with mb_size: {mb_size}, learning_rate: {learning_rate}, epochs: {epochs}\")\n",
    "    \n",
    "    # Create a new model for each configuration\n",
    "    model = create_model()\n",
    "    \n",
    "    # Train the model with the specified parameters and get the final accuracy\n",
    "    final_accuracy = train(model, mb_size=mb_size, learning_rate=learning_rate, epochs=epochs)\n",
    "    \n",
    "    # Check if it's the best model\n",
    "    if final_accuracy > best_accuracy:\n",
    "        best_accuracy = final_accuracy\n",
    "        best_hyperparams = (mb_size, learning_rate, epochs)\n",
    "        best_model = model  # Store the best model\n",
    "\n",
    "# Print the results of the best model\n",
    "print(f\"\\nBest model found with mb_size: {best_hyperparams[0]}, learning_rate: {best_hyperparams[1]}, epochs: {best_hyperparams[2]}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho es: d el valor real es:d\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test[idx].reshape(28,28))\n",
    "pred = best_model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
